{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cbd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from huggingface_hub import (\n",
    "    create_repo,\n",
    "    Repository\n",
    ")\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7402c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORG_ID = \"aieng-lab\"\n",
    "RESULTS_BASE_PATH = \"./results/finetuning\"\n",
    "TEMP_BASE_PATH = \"./results/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d691dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLASH_ATTENTION = [\"ModernBERT\", \"Llama-3.2\", \"CodeLlama\", \"starcoder2\"]\n",
    "\n",
    "# TODO: Make something similar for licensed models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df = pd.read_csv(\"../assets/models.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309c3536",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_type = \"fill-mask\"  # None, multi_label_classification, regression, token-classification, fill-mask\n",
    "task_name = \"requirement_completion\"\n",
    "num_labels = 1\n",
    "\n",
    "model_names = [\"bert-base-cased\", \"bert-large-cased\", \"roberta-base\", \"roberta-large\", \"ModernBERT-base\", \"ModernBERT-large\",\n",
    "               \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\", \"Llama-3.2-1B\", \"Llama-3.2-3B\",\n",
    "               \"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\",\n",
    "               \"codebert-base\",\n",
    "               \"CodeLlama-7b-hf\", \"starcoder2-3b\", \"starcoder2-7b\",\n",
    "               \"codet5p-220m\", \"codet5p-770m\"\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba2580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = \"\"\"---\n",
    "library_name: transformers\n",
    "license: mit\n",
    "language:\n",
    "- en\n",
    "metrics:\n",
    "- accuracy\n",
    "- perplexity\n",
    "base_model:\n",
    "- {base_model_id}\n",
    "pipeline_tag: {problem_type}\n",
    "---\n",
    "\n",
    "# {base_model_name} for filling user actions in requirement specifications\n",
    "\n",
    "This model fills masks ([MASK]) in requirements specifications. During the fine-tuning process, POS verbs were used as a proxy of user actions.\n",
    "\n",
    "- **Developed by:** Fabian C. Peña, Steffen Herbold\n",
    "- **Finetuned from:** [{base_model_id}](https://huggingface.co/{base_model_id})\n",
    "- **Paper:** [Evaluating Large Language Models on Non-Code Software Engineering Tasks](https://arxiv.org/abs/2506.10833)\n",
    "- **Replication kit:** [https://github.com/aieng-lab/senlp-benchmark](https://github.com/aieng-lab/senlp-benchmark)\n",
    "- **Language:** English\n",
    "- **License:** MIT\n",
    "\n",
    "## Cite as\n",
    "\n",
    "```\n",
    "@misc{{peña2025evaluatinglargelanguagemodels,\n",
    "  title={{Evaluating Large Language Models on Non-Code Software Engineering Tasks}}, \n",
    "  author={{Fabian C. Peña and Steffen Herbold}},\n",
    "  year={{2025}},\n",
    "  eprint={{2506.10833}},\n",
    "  archivePrefix={{arXiv}},\n",
    "  primaryClass={{cs.SE}},\n",
    "  url={{https://arxiv.org/abs/2506.10833}}, \n",
    "}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c1a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, flash_attention):\n",
    "    if problem_type == \"token-classification\":\n",
    "        return AutoModelForTokenClassification.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            attn_implementation=\"flash_attention_2\" if flash_attention else None\n",
    "        )\n",
    "    elif problem_type == \"fill-mask\":\n",
    "        return AutoModelForMaskedLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            attn_implementation=\"flash_attention_2\" if flash_attention else None\n",
    "        )\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path,\n",
    "        num_labels=num_labels,\n",
    "        problem_type=problem_type,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\" if flash_attention else None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Models to push:\", len(model_names))\n",
    "for model_name in tqdm(model_names):\n",
    "    print(\"Working on model:\", model_name)\n",
    "\n",
    "    model_path = os.path.join(RESULTS_BASE_PATH, task_name, model_name, \"test\", \"best\")\n",
    "    print(\" - Model path:\", model_path)\n",
    "\n",
    "    try:\n",
    "        with open(os.path.join(model_path, \"config.json\")) as f:\n",
    "            config = json.load(f)\n",
    "        base_model_id = config[\"_name_or_path\"]\n",
    "        print(\" - Base model ID:\", base_model_id)\n",
    "\n",
    "        # Checking whether model has flash attention support\n",
    "        flash_attention = any([(attn in base_model_id) for attn in FLASH_ATTENTION])\n",
    "\n",
    "        # Loading model and tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "        model = load_model(model_path, flash_attention)\n",
    "\n",
    "        # (Workaround) Updating config\n",
    "        model.config._name_or_path = base_model_id\n",
    "\n",
    "        # Testing the model before pushing\n",
    "        if problem_type == \"token-classification\":\n",
    "            problem_type_ = problem_type\n",
    "        elif problem_type == \"fill-mask\":\n",
    "            problem_type_ = problem_type\n",
    "        else:\n",
    "            problem_type_ = \"text-classification\"\n",
    "\n",
    "        pipeline_ = pipeline(\n",
    "            problem_type_,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=0\n",
    "        )\n",
    "        if problem_type == \"fill-mask\":\n",
    "            # print(\"- Test inference:\", pipeline_(\"Hello [MASK]!\"))\n",
    "            print(\"- Test inference:\", pipeline_(\"Hello <mask>!\"))\n",
    "        else:\n",
    "            print(\"- Test inference:\",pipeline_(\"Hello world!\"))\n",
    "\n",
    "        new_model_id = f\"{model_name}_{task_name.replace(\"_\", \"-\")}\"\n",
    "        print(\"- New model ID:\", new_model_id)\n",
    "\n",
    "        new_model_path = os.path.join(TEMP_BASE_PATH, task_name, new_model_id)\n",
    "        print(\"- New model path:\", new_model_path)\n",
    "\n",
    "        # Moving the model and tokenizer\n",
    "        create_repo(repo_id=f\"{ORG_ID}/{new_model_id}\", repo_type=\"model\", private=False)\n",
    "        repo = Repository(local_dir=new_model_path, clone_from=f\"{ORG_ID}/{new_model_id}\")\n",
    "        tokenizer.save_pretrained(new_model_path)\n",
    "        model.save_pretrained(new_model_path)\n",
    "\n",
    "        # Creating model card\n",
    "        base_model_id_ = base_model_id.split(\"/\")\n",
    "        if len(base_model_id_) > 1:\n",
    "            base_model_id_ = base_model_id_[1]\n",
    "        else:\n",
    "            base_model_id_ = base_model_id_[0]\n",
    "        base_model_name = models_df.loc[models_df[\"Model ID\"] == base_model_id_, \"Model name\"].values[0]\n",
    "        model_card_ = model_card.format(base_model_id=base_model_id, problem_type=problem_type_, base_model_name=base_model_name)\n",
    "        # print(\"- Model card:\", model_card_)\n",
    "        with open(os.path.join(new_model_path, \"README.md\"), \"w\") as f:\n",
    "            f.write(model_card_)\n",
    "\n",
    "        # Pushing to the hub\n",
    "        #model.push_to_hub(f\"{ORG_ID}/{new_model_id}\", commit_message=\"Uploading model\")\n",
    "        #tokenizer.push_to_hub(f\"{ORG_ID}/{new_model_id}\", commit_message=\"Uploading tokenizer\")\n",
    "        repo.push_to_hub(commit_message=\"Pushing model, tokenizer and model card\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"- Model not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20763269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
